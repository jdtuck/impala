\documentclass{beamer}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Statistical Calibration of Material Model Parameters: A Hierarchical Design}
\author{Peter Trubey}
\institute[UCSC]{University of California, Santa Cruz}

\usetheme[unclass]{LANL-classic}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Overview}
  \tableofcontents
\end{frame}

\section{Introduction}
\begin{frame}
  \frametitle{Material Models}
  Material models serve to predict how materials will "react" when we impose
  conditions upon them.  There are various models for different properties of a
  material.

  Relevant to our discussion is the flow stress model, which predicts the plastic
  deformation of a material under a given strain.  The model we are using, is
  Preston-Tonks-Wallace.

  PTW, as a model, requires at least 8 parameters be calibrated.
\end{frame}

\begin{frame}
  \frametitle{Model Calibration}
  \framesubtitle{The Stress Strain Curve}

\end{frame}

\section{Methodology}
\subsection{Pooled Model}
\begin{frame}
  \frametitle{Pooled Model}
  \begin{equation*}
    \begin{aligned}
      {\bf y}_i &\sim \mathcal{N}\left(\eta(t_i, \phi), I_{n_i}\sigma^2\right) \hspace{1cm}\text{for }i = 1,\ldots,N
      \phi &\sim \text{I}(\phi \in B)\text{I}(\phi \in C)
      \text{P}(\theta_i\mid {\bf Y}, {\bf \simgma}^2)
      \sigma^2 &\sim \text{IG}(\alpha,\beta)
      \end{aligned}
  \end{equation*}
  \begin{description}
    \item[$\phi$]: the real parameters associated with the material model
    \item[$B$]: a set of parameter bounds that we feel the real parameters \emph{should} be inside
    \item[$C$]: a set of parameter constraints that, if violated, cause \emph{non-physics-y} behavior
    \item[${\bf y}_i$]: the stress values observed
    \item[$t_i$]: the strain rate, material starting temperature, and measured strains at which stresses are observed
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{Pooled Model, Cont.}
  The described model is not going to follow any known form.  Exploring its
  posterior will require an elaborate sampling step.  We use Metropolis Hastings.

  $\phi$ is a little unwieldy.  Each component $\phi_i$ is subject to a different
  set of bounds.  For sampling, rather than sampling on $\phi$, we transform the
  bounds of $\phi$ to a probit scale and sample on $\theta$, the parameters
  transformed into that scale.

  Thus $\theta$ is unbounded.  We still have to contend with $C$, the
  \emph{keep it physics-y} constraint, but that is easy to do within the model
  and within the Metropolis step.
\end{frame}

\begin{frame}
  \frametitle{Pooled Model, Cont.}
  We arrive at the posterior:
  \begin{equation*}
    \begin{aligned}
      \theta\mid{\bf Y}, \sigma^2 &\sim \prod_{i = 1}^N\mathcal{N}\left({\bf y}_i\mid \eta(t_i, \theta), I_{n_i}\sigma^2\right)\lVert J \rVert I(\theta \in C)\\
      \sigma^2\mid {\bf Y}, \theta &\sim \text{IG}\left(\frac{1}{2}\sum_{i = 1}^N n_i + \alpha, \frac{1}{2}\sum_{i = 1}^N\sum_{j = 1}^n_i[y_ij - \eta(t_{ij}, \theta)]^2 + \beta \right)
    \end{aligned}
  \end{equation*}
  We have reason to think this posterior might be multimodal--we'll get to how
  we deal with that later.

  This is the "pooled" model mentioned.  If fitted to each experiment, it
  produces distributions of $\theta$ that do not overlap for some experiments.
  If fitted to all experiments, it produces estimates which do not cover some
  experiments.

  This is a problem. Enter the hierarchical model.
\end{frame}

\subsection{Hierarchical Model}

\begin{frame}
  \frametitle{Hierarchical Model}
  As stated before, the pooled model might have a problem.  That is,
  idiosyncratic differences in experiments (be it material chemistry,
  pre-experiment tempering, random chance) may cause wildly different
  experimental results.
\end{frame}

\begin{frame}
  \frametitle{Hierarchical Model, Cont.}
  \begin{equation*}
    \begin{aligned}
      {\bf y}_i \mid \theta_i, \sigma_i^2 &\sim \mathcal{N}\left({\bf y}_i\mid \eta(t_i,\theta_i),I\sigma_i^2\right)\hspace{1cm}\text{for }i = 1,\ldots,N
      \sigma_i^2 &\sim \text{IG}(\alpha,\beta)
      \text{P}(\theta_i \mid \theta_0,\Sigma) &\propto \mathcal{N}(\theta_i\mid\theta_0, \Sigma)\lVert J\rVertI(\theta \in C)
      \text{P}(\theta_0) &\propto \mathcal{N}(\mu, \Sigma_0)I(\theta_0\in C)
      \Sigma &\sim \text{IW}(\nu, \psi)
    \end{aligned}
  \end{equation*}
  That is, we assume each experiment has a different parameter set, and those
  parameter sets descend from a global mean.
\end{frame}

\begin{frame}
  \frametitle{Hierarchical Model, Cont.}
  Through a bit of math, we arrive at a posterior:
  \begin{equation*}
    \begin{aligned}
      \text{P}(\theta_i\mid \sigma_i^2, \theta_0, \Sigma, {\bf y}_i) &\propto \exp\left(-\frac{1}{2}[\frac{1}{\sigma_i^2}\sum_{l = 1}^{n_i}(y_{il} - \eta(t_{il},\theta_i))^2 + (\theta_i - \theta_0)^t\Sigma^{-1}(\theta_i - \theta_0)]\right)\lVert J \rVertI(\theta_i\in C)\\
      \text{P}(\sigma_i^2 \mid \theta_i) &= \text{IG}\left(\frac{1}{2} n_i + \alpha, \frac{1}{2}\sum_{l = 1}^{n_i}(y_i - \eta(t_i,\theta_i))^2\right)
      \text{P}(\theta_0\mid \theta_i, \Sigma) &\propto \mathcal{N}\left((N\Sigma^{-1} + \Sigma_0^{-1})^{-1}(N\Sigma^{-1}\bar{\theta} + \Sigma_0^{-1}\mu)\right)
      \text{P}(\Sigma\mid \theta_i,\theta_0) &= \text{IW}\left(N + \nu, \sum_{i = 1}^N(\theta_i - \theta_0)(\theta_i - \theta_0)^t + \psi\right)
    \end{aligned}
  \end{equation*}
\end{frame}

\subsection{Parallel Tempering}

\begin{frame}
  \frametitle{Parallel Tempering}
  For both the pooled model and the hierarchical model, we have cause to
  believe that the posterior \emph{might} be multimodal.

  Sampling from a multimodal model presents difficulties.  One approach is
  parallel tempering.

  With parallel tempering, we spawn multiple copies of the sampler that
  implements the above models.  We assign to each copy of this sampler a
  temperature, increasing from 1 (the cold chain).

  For sampling, the likelihood portion of the posterior is exponentiated to
  the reciprocal of the temperature assigned to the chain. (more on that later)

  Every $k$ iterations, we have the chains attempt to swap states, taking the
  form of a Metropolis step with the following probability of acceptance:
  \begin{equation*}
    \alpha = \min \left[1, \frac{\text{P}({\bf S}_2\mid T_1)\text{P}({\bf S}_1\mid T_2)}{\text{P}({\bf S}_1\mid T_1)\text{P}({\bf S}_2\mid T_2)}\right]
  \end{equation*}
  If a random uniform is greater than $\alpha$, then chain 1 will adopt state 2,
  and chain 2 will adopt state 1.
\end{frame}

\subsection{Localized Covariance}

\begin{frame}
  \frametitle{Adaptive Metropolis}
  Tuning of the proposal density is required for efficient sampling using the
  Metropolis Hastings algorithm.  This is especially true using a joint proposal
  (where multiple parameters are proposed at once).

  Adaptive Metropolis is a field of methods that seek to automatically tune this
  proposal density based on how the sampler has performed thus far.  We employ a
  novel method for Adaptive Metropolis, particularly suited to parallel tempering
  and with multimodal distributions.
\end{frame}

\begin{frame}
  \frametitle{Localized Covariance}


\end{frame}



\section{Results}
\begin{frame}
  \frametitle{Pooled Model}
\end{frame}


\section{Looking Forward}
\begin{frame}
  \frametitle{Cluster Model}



% EOF
